# -*- coding: utf-8 -*-
"""UniMiB_SHAR_ADL_load_dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U1EY6cZsOFERD3Df1HRqjuTq5bDUGH03

#UniMiB_SHAR_ADL_load_dataset.ipynb. 
Loads the A-9 (ADL) portion of the UniMiB dataset from the Internet repository and converts the data into numpy arrays while adhering to the general format of the [Keras MNIST load_data function](https://keras.io/api/datasets/mnist/#load_data-function).

Arguments: tbd
Returns: Tuple of Numpy arrays:   
(x_train, y_train),(x_validation, y_validation)\[optional\],(x_test, y_test) 

* x_train\/validation\/test: containing float64 with shapes (num_samples, 151, {3,4,1})
* y_train\/validation\/test: containing int8 with shapes (num_samples 0-9)

The train/test split is by subject

Example usage:  
x_train, y_train, x_test, y_test = unimib_load_dataset()

Additional References  
 If you use the dataset and/or code, please cite this paper (downloadable from [here](http://www.mdpi.com/2076-3417/7/10/1101/html))

Developed and tested using colab.research.google.com  
To save as .py version use File > Download .py

Author:  Lee B. Hinkle, IMICS Lab, Texas State University, 2021

<a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.


TODOs:
* Fix document strings
* Assign names to activities instead of numbers
"""
import torch
import matplotlib.pyplot as plt
import sys
# sys.path.insert(0, '../../src/signal/')
# sys.path.insert(0, '../../src/signal/modules/')
from modules.modules1D_cls_free import Unet1D_cls_free, GaussianDiffusion1D_cls_free

from sklearn.utils import shuffle
from tsai.models.utils import *
from tsai.basics import *
from tsai.inference import load_learner
from tsai.all import *
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import train_test_split

import tensorflow as tf
from tensorflow import keras
import os
import shutil #https://docs.python.org/3/library/shutil.html
from shutil import unpack_archive # to unzip
#from shutil import make_archive # to create zip for storage
import requests #for downloading zip file
from scipy import io #for loadmat, matlab conversion
import pandas as pd
import numpy as np
#import matplotlib.pyplot as plt # for plotting - pandas uses matplotlib
from tabulate import tabulate # for verbose tables
from tensorflow.keras.utils import to_categorical # for one-hot encoding

#credit https://stackoverflow.com/questions/9419162/download-returned-zip-file-from-url
#many other methods I tried failed to download the file properly
def load_psg_data():
    input_dir = '../PSG/'
    x_train = np.load(input_dir +'x_train.npy')
    x_validation = np.load(input_dir +'x_valid.npy')
    x_test = np.load(input_dir +'x_test.npy')
    y_train = np.load(input_dir +'y_train.npy')
    y_validation = np.load(input_dir +'y_valid.npy')
    y_test = np.load(input_dir +'y_test.npy')
    
    return x_train, y_train, x_validation, y_validation, x_test, y_test

def train_model_lstm(x_train, y_train, x_valid, y_valid):
    X, y, splits = combine_split_data([x_train, x_valid], [y_train, y_valid])
    tfms  = [None, TSClassification()] # TSClassification == Categorize
    batch_tfms = TSStandardize()
    dls = get_ts_dls(X, y, splits=splits, tfms=tfms, batch_tfms=batch_tfms, bs=[64, 32])
    #dls.dataset
    model = build_ts_model(LSTM_FCNPlus, dls=dls)
    learn = Learner(dls, model, metrics=accuracy)
    learn.fit_one_cycle(10, lr_max=1e-2)
    return learn

def train_model_cnn(x_train, y_train, x_valid, y_valid):
    X, y, splits = combine_split_data([x_train, x_valid], [y_train, y_valid])
    tfms  = [None, TSClassification()] # TSClassification == Categorize
    batch_tfms = TSStandardize()
    dls = get_ts_dls(X, y, splits=splits, tfms=tfms, batch_tfms=batch_tfms, bs=[64, 32])
    #dls.dataset
    model = build_ts_model(InceptionTimePlus, dls=dls)
    learn = Learner(dls, model, metrics=accuracy)
    learn.fit_one_cycle(20, lr_max=1e-2)
    return learn

def train_model_tst(x_train, y_train, x_valid, y_valid):
    X, y, splits = combine_split_data([x_train, x_valid], [y_train, y_valid])
    tfms  = [None, TSClassification()] # TSClassification == Categorize
    batch_tfms = TSStandardize()
    dls = get_ts_dls(X, y, splits=splits, tfms=tfms, batch_tfms=batch_tfms, bs=[64, 32])
    #dls.dataset
    model = build_ts_model(TSTPlus, dls=dls)
    learn = Learner(dls, model, metrics=accuracy)
    learn.fit_one_cycle(25, lr_max=1e-3)
    return learn

def run_model(x_test, y_test, learn):
    probas, target, preds = learn.get_X_preds(x_test, y_test)
    preds_labels = np.argmax(preds, axis=1)
    target_labels = np.argmax(target, axis=-1) # undo one-hot encoding
    return preds_labels, target_labels


def extract_one_class(class_id):
    one_class_train_data = []
    one_class_train_labels = []
    for i, label in enumerate(y_train):
        class_label = np.argmax(label)
        if class_label == class_id:
            one_class_train_data.append(x_train[i])
            one_class_train_labels.append(label)
    one_class_train_data = np.array(one_class_train_data)
    one_class_train_labels = np.array(one_class_train_labels)
    return one_class_train_data, one_class_train_labels

if __name__ == "__main__":
    print("Get PSG-Audio (npy data)")
    x_train, y_train, x_valid, y_valid, x_test, y_test \
                             = load_psg_data()
    headers = ("Array","shape", "data type")
    mydata = [("x_train:", x_train.shape, x_train.dtype),
            ("y_train:", y_train.shape, y_train.dtype),
            ("x_valid:", x_valid.shape, x_valid.dtype),
            ("y_valid:", y_valid.shape, y_valid.dtype),
            ("x_test:", x_test.shape, x_test.dtype),
            ("y_test:", y_test.shape, y_test.dtype)]
    print("\n",tabulate(mydata, headers=headers))
    print ('\n','-'*72) # just a dashed line
    # x_train_total = np.concatenate((x_train, x_validation), axis = 0)
    # y_train_total = np.concatenate((y_train, y_validation), axis = 0)
    # print(x_train_total.shape, y_train_total.shape)
    print("Num GPUs Available: ", len(tf.config.experimental.list_physical_devices('GPU')))
    
    pad_width = [(0, 0), (0, 512 - x_train.shape[1]), (0, 0)]
    # Pad the array with zeros
    x_train = np.pad(x_train, pad_width, mode='constant', constant_values=0)
    x_valid = np.pad(x_valid, pad_width, mode='constant', constant_values=0)
    x_test = np.pad(x_test, pad_width, mode='constant', constant_values=0)
    #sample_size = 1000
    x_train_new = []
    y_train_new = []
    device = "cuda:3"
    model = Unet1D_cls_free(
        dim = 64,
        dim_mults = (1, 2, 4, 8),
        num_classes = y_train.shape[1],
        cond_drop_prob = 0.5,
        channels = x_train.shape[2])
    ckpt = torch.load("checkpoint/DDPM1D_cls_free_PSG/checkpoint.pt")
    model.load_state_dict(ckpt['model_state_dict'])
    model = model.to(device)
    diffusion = GaussianDiffusion1D_cls_free(
            model,
            seq_length = 512,
            timesteps = 1000).to(device)
    lstm_file = 'lstm_file.txt'
    cnn_file = 'cnn_file.txt'
    tst_file = 'tst_file.txt'
    lstm_acc_all = []
    cnn_acc_all = []
    tst_acc_all = []
    for shuffle_id in range(10):
        x_train_new = []
        y_train_new = []
        for class_id in range(y_train.shape[1]):
            x_train_id, y_train_id = extract_one_class(class_id)
            #x_train_id = x_train_id.reshape(x_train_id.shape[0], x_train_id.shape[2], series_length)
            
            sample_size = int(x_train_id.shape[0]/10)
            x_train_augment = None
            y_train_augment = None
            for i in range(10):
    
                y = torch.Tensor([class_id] * sample_size).long().to(device)
                x_train_id_new = diffusion.sample(classes = y, cond_scale = 3.).cpu().numpy()
                x_train_id_new = x_train_id_new.reshape(x_train_id_new.shape[0], x_train_id_new.shape[2], x_train_id_new.shape[1])
                y_train_id_new = [y_train_id[0]]*sample_size
                y_train_id_new = np.array(y_train_id_new)
                print(x_train_id_new.shape, y_train_id_new.shape)
                if x_train_augment is None:
                    x_train_augment = x_train_id_new
                else:
                    x_train_augment = np.concatenate([x_train_id_new, x_train_augment], axis=0)
            
                if y_train_augment is None:
                    y_train_augment = y_train_id_new
                else:
                    y_train_augment = np.concatenate([y_train_id_new, y_train_augment], axis=0)
                
            x_train_augment = np.concatenate([x_train_id, x_train_augment], axis=0)
            y_train_augment = np.concatenate([y_train_id, y_train_augment], axis=0)
            x_train_new.append(x_train_augment)
            y_train_new.append(y_train_augment)
            #print(x_train_new.shape, y_train_new.shape)
    
        x_train_new = np.concatenate(x_train_new, axis=0)
        y_train_new = np.concatenate(y_train_new, axis=0)
        #x_train_new = x_train_new.reshape(x_train_new.shape[0], series_length, x_train_new.shape[1])
        print(x_train_new.shape, y_train_new.shape)
        x_train_new, y_train_new = shuffle(x_train_new, y_train_new, random_state=42)
    
        # np.save('x_train_diff', x_train_new)
        # np.save('y_train_diff', y_train_new)
        # np.save('x_validation', x_validation)
        # np.save('y_validation', y_validation)
        # np.save('x_test', x_test)
        # np.save('y_test', y_test)
        
    
        learn_lstm = train_model_lstm(x_train_new, y_train_new, x_valid, y_valid)
        preds_labels_lstm, target_labels_lstm = run_model(x_test, y_test, learn_lstm)
        acc_lstm = accuracy_score(target_labels_lstm, preds_labels_lstm)
        lstm_acc_all.append(acc_lstm)
        file = open(lstm_file,'a')
        write_str = str(acc_lstm)+ '--------'
        file.write(write_str)
        file.close()
    
        learn_cnn = train_model_cnn(x_train_new, y_train_new, x_valid, y_valid)
        preds_labels_cnn, target_labels_cnn = run_model(x_test, y_test, learn_cnn)
        acc_cnn = accuracy_score(target_labels_cnn, preds_labels_cnn)
        cnn_acc_all.append(acc_cnn)
        file = open(cnn_file,'a')
        write_str = str(acc_cnn)+ '--------'
        file.write(write_str)
        file.close()
    
        learn_tst = train_model_tst(x_train_new, y_train_new, x_valid, y_valid)
        preds_labels_tst, target_labels_tst = run_model(x_test, y_test, learn_tst)
        acc_tst = accuracy_score(target_labels_tst, preds_labels_tst)
        tst_acc_all.append(acc_tst)
        file = open(tst_file,'a')
        write_str = str(acc_tst)+ '--------'
        file.write(write_str)
        file.close()
    
    print('LSTM_FCN: ', lstm_acc_all)
    print('InceptionTime: ', cnn_acc_all)
    print('TST: ', tst_acc_all)


